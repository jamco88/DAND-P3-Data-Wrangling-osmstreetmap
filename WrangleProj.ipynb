{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 - Data Wrangling (Python, SQL)\n",
    "\n",
    "## James Cooper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Details (Wrangle OpenStreetMap Data):\n",
    "\n",
    "Location: Glasgow - I chose this city as this is where I live. It's a nice city and I'd like to contribute to the clarity of the data associated with it.\n",
    "\n",
    "Objective:\n",
    "\n",
    "Choose any area of the world in https://www.openstreetmap.org and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data for a part of the world that you care about.\n",
    "\n",
    "References:\n",
    "\n",
    "    Udacity \"Data Wrangling with SQL\"\n",
    "    Udacity forums\n",
    "    Stack Overflow\n",
    "\n",
    "Contents:\n",
    "\n",
    "    1 - High Level Overview\n",
    "    2 - Problems in the data: Audit - Streetnames / Phone Number / Postcode\n",
    "    3 - Cleaning the data / Importing to SQL / Creating Database\n",
    "    4 - SQL analysis\n",
    "    5 - Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - High Level Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will set up the file by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the XML file to get a feel for the contents by counting the top level tags.\n",
    "\n",
    "The following code completed as part of the Udacity problem sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 26514,\n",
      " 'nd': 657766,\n",
      " 'node': 577196,\n",
      " 'osm': 1,\n",
      " 'relation': 1061,\n",
      " 'tag': 367900,\n",
      " 'way': 93605}\n"
     ]
    }
   ],
   "source": [
    "#initialise empty dictionary\n",
    "tags = {}\n",
    "def count_tags(filename):\n",
    "        # loop over elementtree object\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag]+= 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    return tags\n",
    "\n",
    "def count():\n",
    "\n",
    "    tags = count_tags('G129.osm')\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "\n",
    "count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OSM wiki explains the basic structure of the data. In this instance we see that there are just short of 600,000 nodes in the dataset - these represent specific points on the map, and have associated attributes embedded in the node tag itself. Some nodes have children - additional information about each node is embedded in the child tags. As this is a top level search these child tags are not returned here.\n",
    "\n",
    "Ways are lists of nodes that make up an area, route or street. Each way also has its own attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSM filesize = 131.786712 MB\n",
      "Sample filesize = 6.665162 MB\n",
      "csv - nodes filesize = 47.936789 MB\n",
      "csv - nodes_tags filesize = 5.368172 MB\n",
      "csv - ways filesize = 5.620808 MB\n",
      "csv - ways_nodes filesize = 16.240482 MB\n",
      "csv - ways_tags filesize = 8.00229 MB\n",
      "database filesize = 94.425088 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print 'OSM filesize = {} MB'.format(os.path.getsize('G129.osm')/1.0e6)\n",
    "print 'Sample filesize = {} MB'.format(os.path.getsize('G20.osm')/1.0e6)\n",
    "print 'csv - nodes filesize = {} MB'.format(os.path.getsize('nodes.csv')/1.0e6)\n",
    "print 'csv - nodes_tags filesize = {} MB'.format(os.path.getsize('nodes_tags.csv')/1.0e6)\n",
    "print 'csv - ways filesize = {} MB'.format(os.path.getsize('ways.csv')/1.0e6)\n",
    "print 'csv - ways_nodes filesize = {} MB'.format(os.path.getsize('ways_nodes.csv')/1.0e6)\n",
    "print 'csv - ways_tags filesize = {} MB'.format(os.path.getsize('ways_tags.csv')/1.0e6)\n",
    "print 'database filesize = {} MB'.format(os.path.getsize('database.db')/1.0e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Tagtypes.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is intended to highlight any issues within the k attribute of each tag. The code was provided in the lesson.\n",
    "Each key is assigned to one of four groups: lower means that the key includes lower case characters.\n",
    "Lower_colon includes colons. problemchars includes any characters such as [=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]. other covers any keys not meeting the above criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 318604, 'lower_colon': 26284, 'other': 23011, 'problemchars': 1}\n"
     ]
    }
   ],
   "source": [
    "filename = 'G129.osm'\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#function to sort tags into categories based on K attribute\n",
    "def key_type(element, keys):\n",
    "    #ensures that we are only looking at the TAGS returned by the Etree parser called tags\n",
    "    if element.tag == \"tag\":\n",
    "        #ensures we are looking at the attribute 'K' within the tag for that same element\n",
    "        k = element.attrib['k']\n",
    "        #applies the regex function to the k tag - if it matches it resolves to TRUE\n",
    "        if re.search(lower, k):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.search(lower_colon, k):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif re.search(problemchars, k):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "        pass\n",
    "\n",
    "    return keys\n",
    "    \n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def maptags():\n",
    "    keys = process_map(filename)\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "maptags()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of the entire dataset only one of the keys includes problematic characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Countuser.py\n",
    "\n",
    "As part of the exercises the following funciton was written in order to return the total number of users who had made modifications to in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n"
     ]
    }
   ],
   "source": [
    "def get_user(element):\n",
    "    #if you see uid in the attributes list of the elements\n",
    "    if \"uid\" in element.attrib:\n",
    "        #this function will produce the number assigned to that attribute\n",
    "        return element.attrib[\"uid\"]\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        uid = get_user(element)\n",
    "        #if there is not nothing in the UID field, add the uid to the output\n",
    "        if uid != None:\n",
    "            users.add(uid)\n",
    "        pass\n",
    "\n",
    "    return len(users)\n",
    "\n",
    "def uniqueusers():\n",
    "\n",
    "    users = process_map('G129.osm')\n",
    "    pprint.pprint(users)\n",
    "    \n",
    "uniqueusers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2 - Problems in the data: Audit - Streetnames / Phone Number / Postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit.py\n",
    "\n",
    "Purpose of this function is to return all values not matching a specific format.\n",
    "The fields I am looking to clean are Streetnames, phone numbers and postcodes.\n",
    "\n",
    "The code builds on that provided in the exercise  - I have added to the expected list for streetnames to reflect that there are several extra common words used in Glasgow to refer to streets, based on the previous instance of this code which was run on a 5% sample of the full dataset. This will ensure that they are not showing up erroneously in the Audit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DICTIONARY OF NONSTANDARD STREET TYPES:'\n",
      "\n",
      "\n",
      "defaultdict(<type 'set'>, {'Glen': set(['Peel Glen']), 'Rigg': set(['Cander Rigg']), 'West': set(['Paisley Road West']), 'Candleriggs': set(['Candleriggs']), 'Cross': set(['Bridgeton Cross']), 'Hutton': set(['Hutton']), 'street': set(['Brand street', 'Ross street', 'Hope street', 'southbank street', 'shaw street', 'West nile street']), 'Toll': set(['The Toll']), 'East': set(['Drumry Road East']), 'Dovehill': set(['Great Dovehill']), 'Quay': set(['Carrick Quay', 'Pacific Quay']), 'Gallowgate': set(['Gallowgate']), 'Road,': set(['Cumbernauld Road,']), 'Park': set(['Claythorn Park', 'Hanson Park', 'Montfort Park']), 'Holdings': set(['Langbank Holdings']), 'Strret': set(['Garfield Strret']), 'Roystonhill': set(['Roystonhill']), 'Bridgegate': set(['Bridgegate']), 'Drygate': set(['Drygate']), 'Westercraigs': set(['Westercraigs']), 'Murchison': set(['Murchison']), 'Grove': set(['East Nerston Grove', 'Murray Grove']), 'Sreet': set(['Springbank Sreet']), 'St': set(['Ardgay St']), 'Quadrant': set(['Wyvis Quadrant']), 'Broomielaw': set(['Broomielaw']), 'Green': set(['Glasgow Green', 'Templeton on the Green']), 'Brae': set(['Newton Brae', 'Aidans Brae']), 'North': set(['Canal Bank North']), 'Saltmarket': set(['Saltmarket']), 'bank': set(['canal bank']), 'Rd': set(['Victoria Rd', 'Farmeloan Rd', 'Baillieston Rd', 'Downiebrae Rd', 'East Kilbride Rd', 'Balmore Rd']), 'Wharf': set(['Spiers Wharf']), 'Parade': set(['Alexandra Parade']), 'Estate': set(['Spiersbridge Lane, Thornliebank Industrial Estate', 'Oakbank Industrial Estate']), 'Circus': set(['Claythorn Circus', 'Broompark Circus']), 'Craigpark': set(['Craigpark']), 'Wynd': set(['Duke Wynd']), 'Walk': set(['Glenraith Walk', 'Mossvale Walk']), 'Centre': set(['Hillington Shopping Centre']), 'Kinfauns': set(['Kinfauns']), 'Trongate': set(['Trongate']), 'Village': set(['Greenlaw Village']), 'Close': set(['Firpark Close', 'Green Close']), 'road': set(['Garscadden road', 'Kinloch road', 'pollokshaws road', 'Marihill road']), 'Row': set(['Monteith Row', 'Water Row'])})\n",
      "\n",
      "\n",
      "'LIST OF NONSTANDARD FORMAT PHONE NUMBERS:'\n",
      "\n",
      "\n",
      "['0141 552 0841',\n",
      " '0141 553 2400',\n",
      " '+44 141 2214452',\n",
      " '+44 141 2214036',\n",
      " '+44 141 8470129',\n",
      " '+44 141 6329417',\n",
      " '+44 141 9426011',\n",
      " '+44 141 5528148',\n",
      " '0141 959 1520',\n",
      " '+441419529999',\n",
      " '+44 3457 242424',\n",
      " '01413316600',\n",
      " '+44 141 222 2989',\n",
      " '0141 958 1055',\n",
      " '+44 141 339 8558',\n",
      " '01412374730',\n",
      " '+44 141 339 2349',\n",
      " '+44 141 554 1830',\n",
      " '0141 553 1249',\n",
      " '0141 556 4430',\n",
      " '+44 141 6199001',\n",
      " '0141 6322733',\n",
      " '+44 141 632 3427',\n",
      " '0141 773 1202',\n",
      " '+44 141-332-3726',\n",
      " '+44 141 353 6602',\n",
      " '+44 141 332 9902',\n",
      " '+44 203 519 1111',\n",
      " '0141 572 1472',\n",
      " '0141 554 1763',\n",
      " '0141 339 0888',\n",
      " '+44 141 204 5555',\n",
      " '0141 248 4427',\n",
      " '0141 221 1555 / 0141 204 1444',\n",
      " '07910 453 508',\n",
      " '00441412211568',\n",
      " '0141 221 8176',\n",
      " '+44 14 1404 2690',\n",
      " '0141 2222 959',\n",
      " '0141 248 3292',\n",
      " '0141 248 5505',\n",
      " '+44 141 5481340',\n",
      " '0141 611 9555',\n",
      " '0141 552 4025',\n",
      " '0141 552 1094',\n",
      " '01415721405',\n",
      " '0141 333 0980',\n",
      " '+44 141 931 5252',\n",
      " '01419432600',\n",
      " '+44 141 222 2255',\n",
      " '0141 552 0753',\n",
      " '+44 141 942 7070',\n",
      " '+44 141 237 7949',\n",
      " '0141 649 8831',\n",
      " '0141 649 4647',\n",
      " '0141 632 2127',\n",
      " '0141 554 3000',\n",
      " '0141 551 8840',\n",
      " '0141 550 0001',\n",
      " '0141 556 2366',\n",
      " '0141 550 1020',\n",
      " '0141 554 3828',\n",
      " '0141 554 1074',\n",
      " '0141 556 6166',\n",
      " '0141 221 7711',\n",
      " '+44 1416199000',\n",
      " '0141 557 3488',\n",
      " '+44 141 554 8668',\n",
      " '0141 553 4060',\n",
      " '0141 944 2229',\n",
      " '0141 944 4195',\n",
      " 'yes',\n",
      " 'yes',\n",
      " '07977 947288',\n",
      " '0141 632 7700',\n",
      " '0141 649 3963',\n",
      " '0141 649 1688',\n",
      " '0141 649 9955',\n",
      " '0141 632 1335',\n",
      " '0141 779 2077',\n",
      " 'yes',\n",
      " '0141 778 2717',\n",
      " 'yes',\n",
      " '0141 944 2063',\n",
      " '0141 946 1234',\n",
      " '+44 141 332 7716; +44 141 332 4221',\n",
      " '+44 141 553 4288',\n",
      " 'yes',\n",
      " '0141 774 5599',\n",
      " '+44 141 554 4558',\n",
      " '0141 770 4330',\n",
      " '0141 770 9028',\n",
      " '0141 77 000 99',\n",
      " '0141 770 7489',\n",
      " '0141 770 999',\n",
      " '0141 770 9442',\n",
      " '0141 770 7638',\n",
      " 'yes',\n",
      " '0141 774 4496',\n",
      " '0141 774 0985',\n",
      " 'yes',\n",
      " 'yes',\n",
      " 'yes',\n",
      " 'yes',\n",
      " 'yes',\n",
      " '0141248333',\n",
      " '0141 778 2002',\n",
      " '0141 332 6229',\n",
      " '0141 945 5959',\n",
      " '01412926261',\n",
      " '0141 557 3355',\n",
      " '+44 (141) 297 1147',\n",
      " '0141 781 2131',\n",
      " '01415521246',\n",
      " '0141 221 5479',\n",
      " 'yes',\n",
      " 'yes',\n",
      " 'yes',\n",
      " 'yes',\n",
      " '+44 141 551 8088',\n",
      " '0141 550 7511',\n",
      " 'yes',\n",
      " '0141 237 3266',\n",
      " '0141 222 2000',\n",
      " '0141 552 3186',\n",
      " 'yes',\n",
      " '0141 221 6000',\n",
      " '00441412585565',\n",
      " '0141 357 6437',\n",
      " '0141 551 8888',\n",
      " '0141 554 3339',\n",
      " '+44 141 554 0792',\n",
      " '+44 141 551 0022',\n",
      " '0141 574 6066',\n",
      " '0141 331 6227',\n",
      " '+44 3457 242424',\n",
      " '0141 248 2228',\n",
      " '0141 243 2459',\n",
      " '0141 332 8513',\n",
      " '0141 352 8800',\n",
      " '0141 221 5518',\n",
      " '0141 229 1427',\n",
      " 'yes',\n",
      " '0141 357 6747',\n",
      " '00441413342740',\n",
      " 'yes',\n",
      " '0141 886 2727',\n",
      " '0141 532 1305',\n",
      " '+44 141 552 7849',\n",
      " '0845 9758758',\n",
      " '0141 334 3571',\n",
      " '0141 353 0875',\n",
      " '0845 459 0078',\n",
      " '+44 141 882 6995',\n",
      " '0141 332 3526',\n",
      " '0141 332 4622',\n",
      " '+447428621596',\n",
      " '0141 332 9129',\n",
      " '01412485409',\n",
      " '0141 637 2182',\n",
      " '+44 0141 226 4092',\n",
      " '0141 950 6350',\n",
      " '0141 950 6029 / 0141 644 1070',\n",
      " '01413397333',\n",
      " '0141 954 8860',\n",
      " '01419547777',\n",
      " '0141 950 1333',\n",
      " '0141 20959 200438',\n",
      " '0141 959 2112',\n",
      " '0141 950 1586',\n",
      " '+44141 954 4488',\n",
      " '0141 954 8280 ',\n",
      " '0141 950 6611',\n",
      " '0141 950 4400',\n",
      " '0141 959 2225',\n",
      " '0141 959 7175',\n",
      " '0141 959 4163',\n",
      " '0141 959 8111',\n",
      " '0141 954 0505',\n",
      " '0141 434 0063',\n",
      " '+44 141 958 1069',\n",
      " '0141 959 3639',\n",
      " '0141 959 9999',\n",
      " '0141 950 4672',\n",
      " '0141 954 8015',\n",
      " '+44 141 959 9545',\n",
      " '01419590004',\n",
      " '0141 944 8007',\n",
      " '0141 9447721',\n",
      " '0141 9448678',\n",
      " '0141 2760682',\n",
      " '01419444248',\n",
      " '0141 9444449',\n",
      " '0141 20944 204902',\n",
      " '0141 9442612',\n",
      " '0141 9444383',\n",
      " '01419441238',\n",
      " '01419445490',\n",
      " '07714594513',\n",
      " '0141 949 0906',\n",
      " '0141 9491114',\n",
      " '0141 9443860',\n",
      " '0141 207 7100',\n",
      " '0141 9445440',\n",
      " '01415504327',\n",
      " '01413 011207',\n",
      " '0141 237 9144',\n",
      " '0141 334 2788',\n",
      " '0141 331 8941',\n",
      " '0141 375 0464',\n",
      " '01413424188',\n",
      " '0141 445 3718',\n",
      " '0141 332 2552',\n",
      " '0141 352 7435',\n",
      " '0141 559 4941',\n",
      " '0141 425 2020',\n",
      " '01413033131',\n",
      " '0141 332 0111',\n",
      " '0141 552 2786',\n",
      " '0141 552 2822',\n",
      " '0141 7734461',\n",
      " '0141 548 1552',\n",
      " '0141 237 7200',\n",
      " '0141 556 6315',\n",
      " '0141 636 6119',\n",
      " '0141 328 3303',\n",
      " '0141 945 2746',\n",
      " '0141 331 7676',\n",
      " '0141 945 1369',\n",
      " '0141 945 4100',\n",
      " '0141 762 4413',\n",
      " '+44 141 243 4400',\n",
      " '+44 141 419 0610',\n",
      " '+44 3457 242424',\n",
      " '01419462631',\n",
      " '+44 141 429 8787',\n",
      " '+44 141 6361131',\n",
      " '01414357727',\n",
      " '01419469020',\n",
      " '01419 452649',\n",
      " '+44 141 886 4413',\n",
      " '0141 946 1124',\n",
      " '0141 9453800',\n",
      " '0141 945 5354',\n",
      " '0141 946 1051',\n",
      " '0141 9466035',\n",
      " '+44 141 353 2959',\n",
      " '0141 237 4391',\n",
      " '+440141 429 4290',\n",
      " '+440141 552 2030',\n",
      " '+4401412488884',\n",
      " '+ 44 (0) 141 353 3708',\n",
      " '0141 267 4092',\n",
      " '0141 296 4092',\n",
      " '+44 141 554 4538',\n",
      " '+44 141 554 8275',\n",
      " '+44 141 554 2925',\n",
      " '+44 141 550 1050',\n",
      " '+44 141 550 2266',\n",
      " '+44 141 551 0444',\n",
      " '01419451778',\n",
      " '01419451647',\n",
      " '01412483495',\n",
      " '+44 141 334 1313',\n",
      " '+44 141 353 1355',\n",
      " '+44 141 532 0415',\n",
      " '+44 141 237 6250',\n",
      " '0141 5525502',\n",
      " '+44 141 532 8000',\n",
      " '+44141 333 9312',\n",
      " '00441413329713',\n",
      " '00443001245099',\n",
      " '0044141 332 5815',\n",
      " '00441414283400',\n",
      " '00441412584179',\n",
      " '01413373419',\n",
      " '01416201003',\n",
      " '0141 649 1684',\n",
      " '+44 141 773 1191',\n",
      " '0141 339 5529',\n",
      " '+44 (0) 141 334 3811',\n",
      " '0141 620 0434',\n",
      " '0141 334 4343',\n",
      " '0141 334 7139',\n",
      " '0141 648 9999',\n",
      " '0141 334 3110',\n",
      " '0141 328 9557',\n",
      " '+44 141 4296701',\n",
      " '+44 141 554 4175',\n",
      " '+441412044106',\n",
      " '+44 141 942 8534',\n",
      " '+44 141 942 4242',\n",
      " '0141847000',\n",
      " '01419597050',\n",
      " '+44 141 207 7700',\n",
      " '+44 141 207 7480',\n",
      " '01418005900',\n",
      " '01412214718',\n",
      " '01412291468',\n",
      " '01413348973',\n",
      " '+44 141 3321263',\n",
      " '+44 141 276 0600',\n",
      " '+44 141 427 1157; +44 141 427 3355 (Pro Shop)',\n",
      " '+44 141 632 4351 (Manager); +44 141 632 1080 (Clubhouse)',\n",
      " '+44 141 6387044',\n",
      " '+441419422349',\n",
      " '+44 141 586 5300',\n",
      " '+44 8433120040',\n",
      " '+44 141 761 1281',\n",
      " '+44 141-772-8938',\n",
      " '0141 9441462',\n",
      " '0141 577 8250',\n",
      " '+44141 330 6704',\n",
      " '+44 844 493 2202',\n",
      " '+441416387988',\n",
      " '+44 141 276 0556',\n",
      " 'Phone 0141 276 0774',\n",
      " '+44 141 339 8811',\n",
      " '0141 20276 200560',\n",
      " '0141 20944 202554',\n",
      " '+44-141-332 3311',\n",
      " '0141 649 2962',\n",
      " '01416365334',\n",
      " '+44 7827 305365',\n",
      " '+44 141 276 0810',\n",
      " '0141 944 203642',\n",
      " '0141 643 3470',\n",
      " '0141 248 8888',\n",
      " '+44-141-4284441',\n",
      " '0141 248 4488',\n",
      " '+44 141 332 2795',\n",
      " '+44 141 559 4331',\n",
      " '+441415545281',\n",
      " '+44 800 977 7766',\n",
      " '0141 552 2424',\n",
      " '01412218733',\n",
      " '0141 552 2324',\n",
      " '+44 (0) 141 225 0430',\n",
      " '0141 554 1500',\n",
      " '0141 556 3202',\n",
      " '0141 552 3834',\n",
      " '0141 221 9444',\n",
      " '+44 141 276 0700',\n",
      " '+44 141 276 0811',\n",
      " '+ 44 (0) 141 345 2140',\n",
      " '0871 527 8442',\n",
      " '0141 558 2020',\n",
      " '+44 141 339-6955',\n",
      " '0141 558 0088',\n",
      " '01412116090',\n",
      " '0141 778 8678',\n",
      " '0141 779 4159',\n",
      " '+44 141 276 0811',\n",
      " '+441416393279',\n",
      " '+441416163144',\n",
      " '+44 141 554 3927',\n",
      " '+441416396358',\n",
      " '+441416398097',\n",
      " '+441416391120',\n",
      " '+44141638354',\n",
      " '+441416160699',\n",
      " '+441416395066',\n",
      " '01416393735',\n",
      " '+441416392718',\n",
      " '+441416399578',\n",
      " '01416390400',\n",
      " '+441416390757',\n",
      " '01416394688',\n",
      " '+441416391996',\n",
      " '+441416393636',\n",
      " '0141 770 8200',\n",
      " '0141 770 9955',\n",
      " '0141 554 6996',\n",
      " '0141 336 8050',\n",
      " '0141 287 9700',\n",
      " '0141 881 7823',\n",
      " '0141 207 7400',\n",
      " '0141 554 3454',\n",
      " '+44 345 026 9554',\n",
      " '+44 345 026 9554',\n",
      " '0141 554 2121',\n",
      " '01412926270',\n",
      " '0141 575 3001',\n",
      " '0141 643 9600',\n",
      " '+44 (0)141 644 3838',\n",
      " '0141 558 2118',\n",
      " '+44 141 876 0458',\n",
      " '0141 883 0624',\n",
      " '0141 773 0038',\n",
      " '0141 618 4895',\n",
      " '+441416372212',\n",
      " '+441416380060',\n",
      " '+44 141 810 1011',\n",
      " '+44 141 570 4080',\n",
      " '0141 641 3130',\n",
      " '+44 07530795668',\n",
      " '01412926272',\n",
      " '+441415504327',\n",
      " '01412926276',\n",
      " '01412926274',\n",
      " '01412926279',\n",
      " '+44 141 276 1614',\n",
      " '0141 944 4383',\n",
      " '0141 944 2612',\n",
      " '0141 332 0041',\n",
      " '+44 141 952 6761',\n",
      " '0300 300 0250',\n",
      " '0141 886 1755',\n",
      " '+44 141 886 3377',\n",
      " '+44 141 892 0076',\n",
      " '+44 141 647 8572',\n",
      " '+44 141 554 1830',\n",
      " '+441419441767',\n",
      " '+441416378750',\n",
      " '+441416476967',\n",
      " '+441416475932',\n",
      " '+441415820120',\n",
      " '+44 141 942 4278',\n",
      " '+44 (0)141 644 3511',\n",
      " '+441415707280',\n",
      " '+441418823602',\n",
      " '+441414279334',\n",
      " '+441416386135',\n",
      " '+441389872068',\n",
      " '+441418823451',\n",
      " '+441416476760',\n",
      " '+441416412218',\n",
      " '+441416392922',\n",
      " '+441415707020',\n",
      " '+441416320745',\n",
      " '+441414270375',\n",
      " '+441414237733',\n",
      " '+441418832010',\n",
      " '+441414455884',\n",
      " '+441419450633',\n",
      " '+441419552276',\n",
      " '+441418805305',\n",
      " '+441418812424',\n",
      " '+441418821915',\n",
      " '+441416412472',\n",
      " '+441416413344',\n",
      " '+441417732052',\n",
      " '+441417789616',\n",
      " '01417620800']\n",
      "\n",
      "\n",
      "'LIST OF NONSTANDARD FORMAT POSTCODES:'\n",
      "\n",
      "\n",
      "['G51',\n",
      " 'PA4 8RP',\n",
      " 'PA4 8QG',\n",
      " 'PA4 8QJ',\n",
      " 'PA4 8QE',\n",
      " 'G22',\n",
      " 'PA4 8QU',\n",
      " 'PA4 8QY',\n",
      " 'G76',\n",
      " 'PA4 8XQ',\n",
      " 'PA4 8RU',\n",
      " 'PA4 8RU',\n",
      " 'G42',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'G64',\n",
      " 'PA4 0NQ',\n",
      " 'PA4 8XQ',\n",
      " 'PA4 8XQ',\n",
      " 'G21',\n",
      " 'G21',\n",
      " 'G21',\n",
      " 'PA4 8RU',\n",
      " 'PA4 8QP',\n",
      " 'PA4 8YX',\n",
      " 'PA4 8QL',\n",
      " 'PA4 8QD',\n",
      " 'PA4 0AN',\n",
      " 'PA4 8TT',\n",
      " 'PA4 0AJ',\n",
      " 'PA4 8PF',\n",
      " 'PA4 8XT',\n",
      " 'PA4 0DJ',\n",
      " 'PA4 8ND',\n",
      " 'PA4 8QL',\n",
      " 'PA4 8SH',\n",
      " 'PA1 3AT']\n"
     ]
    }
   ],
   "source": [
    "osmfile = \"G129.osm\"\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "regex1 = re.compile(r'\\+44\\-\\d{3}\\-\\d{3}\\-\\d{4}')\n",
    "\n",
    "regex2 = re.compile(r'^[gG]\\d\\d?[ _-]?\\d\\w\\w')\n",
    "\n",
    "# updated to account for local street type conventions as a result of testing sample \"G20.osm\"\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Crescent\", \"Gate\", \"Gardens\", \"Path\", \"Terrace\", \"Way\"]\n",
    "\n",
    "# initial mapping dictionary - this will be updated in data.py\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\"\n",
    "            }\n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_phone_number(elem):\n",
    "    if \"phone\" in elem.attrib['k'].lower():\n",
    "        return (elem.attrib['v'])  \n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit_number_type(badnumlist, number):\n",
    "\tgoodmatch = regex1.match(number)\n",
    "\tif not goodmatch:\n",
    "\t\tbadnumlist.append(number)\n",
    "\n",
    "def audit_postcode(badpclist, postcode):\n",
    "    goodmatch2 = regex2.match(postcode)\n",
    "    if not goodmatch2:\n",
    "        badpclist.append(postcode)\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    badnumlist = []\n",
    "    badpclist = []\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "                elif is_phone_number(tag):\n",
    "\t\t\t\t\taudit_number_type(badnumlist, tag.attrib['v'])\n",
    "                elif is_postcode(tag):\n",
    "                    audit_postcode(badpclist, tag.attrib['v'])\n",
    "                \n",
    "    osm_file.close()\n",
    "    pprint.pprint(\"DICTIONARY OF NONSTANDARD STREET TYPES:\")\n",
    "    print \"\\n\"\n",
    "    pprint.pprint(street_types)\n",
    "    print \"\\n\"\n",
    "    pprint.pprint(\"LIST OF NONSTANDARD FORMAT PHONE NUMBERS:\")\n",
    "    print \"\\n\"\n",
    "    pprint.pprint(badnumlist)\n",
    "    print \"\\n\"\n",
    "    pprint.pprint(\"LIST OF NONSTANDARD FORMAT POSTCODES:\")\n",
    "    print \"\\n\"\n",
    "    pprint.pprint(badpclist)\n",
    "\n",
    "\t#pprint.pprint(street_types)\n",
    "    #pprint.pprint(badnumlist)\n",
    "    #pprint.pprint(badpclist)\n",
    "audit(osmfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Types\n",
    "\n",
    "At first glance, based on the number of returns it looks like the initial expected list was not sufficiently comprehensive - however many of these results are not \"Wrong\" per se, so much as \"reflective of the rich linguistic heritage of scotland\". These results do not require cleaning and can be left in the data. However there are also some problems with spelling and consistency (Sreet, Strret etc) - these are errors that can be fixed by adding additional terms to our mapping dictionary in data.py using it to swap in something legible wherever one of these issues is encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Numbers\n",
    "\n",
    "The returned numbers fall into several categories.\n",
    "\n",
    "1. There are various permutations of no spaces, spaces and dashes used at different points in each number.\n",
    "2. Some numbers include an international dialling code. Others omit this and are in local format.\n",
    "3. Some have both the international dialing code and the local code fully included within the number \n",
    "4. Some phone numbers include brackets or spaces between the international and local codes.\n",
    "5. Some numbers include multiple separate phone numbers.\n",
    "6. Some numbers just have the wrong number of numbers.\n",
    "\n",
    "To ensure consistency I decided to impose the format (+44-nnn-nnn-nnnn). +44 is the international dialing code for the uk and replaces the first 0 in the local number.\n",
    "\n",
    "To address these points, my function in data.py will take each value and remove all non digits. This removes a lot of the degrees of entropy when dealing with the above issues. From there, we can check the length of the string of the  number and use .startswith in order to match each of the various circumstances identified above. Following this, the number is modified to match my specific format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcodes\n",
    "\n",
    "Not much to report here - other than the obvious observation that there a few PA2 postcodes in there. These correspond to addresses in renfrewshire. It was in fact myself who inadvertently caught a section of renfrewshire in the map when selecting a square map area that both met the required filesize and covered the glasgow metropolitan area. These won't impair anyone's ability to use the dataset provided they are aware of how it was selected, so I will leave them in.\n",
    "\n",
    "With regards to the other results - several results are returning only the high level \"Area\" section of the postcode, IE only the first 2 or 3 characters. To quote wikipedia:\n",
    "\n",
    "\"The structure of a postcode is a one or two-letter postcode area code named for a local town or area of London, one or two digits signifying a district in that region, a space, and then an arbitrary code of one number and two letters. For example, the postcode of the University of Roehampton in London is SW15 5PU, where SW stands for south-west London. The postcode of GCHQ is GL51 0EX, where GL signifies the postal town of Gloucester.\"\n",
    "\n",
    "In this case as the area of interest is Glasgow, the postcode area code is G. The district code can be one or two digits, and the arbitrary code (after the space or dash) giving the exact delivery address is denoted by Number Letter Letter. So overall some of our returns are only showing the District code - unfortunately there is no way to fix this. I will not remove these however as they still give some indication of where the node is located, which is better than nothing.\n",
    "\n",
    "Note - I also intend to create Area codes for nodes that already have postcodes. This is because it allows us to search by district, to get a sense of where all the good amenities are located for example.\n",
    "\n",
    "My code will also modify all existing postcode consisting of 2 or 3 characters to have \"---\" appended to them so that it is clear that they are incomplete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 - Cleaning the data / Exporting to CSV / Creating Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data.py The following code was used to complete these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_PATH = \"G129.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "        if element.tag == 'node':\n",
    "        for attribute in NODE_FIELDS:\n",
    "            #if key exists\n",
    "            if element.attrib.get(attribute):\n",
    "                node_attribs[attribute] = element.attrib[attribute]\n",
    "            else:\n",
    "                #insert placeholder value for missing data\n",
    "                node_attribs[attribute] = \"999999999999\"\n",
    "        for child in element:\n",
    "            nodes = {}\n",
    "            #create a new dictionary that can be updated with the new tag Areacode without overwriting the postcode\n",
    "            nodes1 = {}\n",
    "            problem = PROBLEMCHARS.match(child.attrib['k'])\n",
    "            lowco = LOWER_COLON.match(child.attrib['k'])\n",
    "            if problem:\n",
    "                continue\n",
    "            elif \"phone\" in child.attrib['k'].lower():\n",
    "                nodes['type'] = 'regular'\n",
    "                nodes['key'] = child.attrib['k']\n",
    "                nodes['id'] = element.attrib['id']\n",
    "                if update_phone(child.attrib['v']):\n",
    "                    nodes[\"value\"] = update_phone(child.attrib['v'])\n",
    "                    tags.append(nodes) \n",
    "            elif lowco:\n",
    "                nodes['type'] = child.attrib['k'].split(':', 1)[0]\n",
    "                nodes['key'] = child.attrib['k'].split(':', 1)[1]\n",
    "                nodes['id'] = element.attrib['id']\n",
    "                if child.attrib['k'] == \"addr:street\":\n",
    "                    nodes[\"value\"] = update_name(child.attrib['v'], mapping)\n",
    "                    tags.append(nodes)\n",
    "                elif child.attrib['k'] == \"addr:postcode\":\n",
    "                    nodes[\"value\"] = update_postcode(child.attrib['v'])\n",
    "                    tags.append(nodes)\n",
    "                    if make_areacode_out_of_postcode(child.attrib['v']):\n",
    "                        nodes1['type'] = 'addr'\n",
    "                        nodes1['key'] = 'Areacode'\n",
    "                        nodes1['id'] = element.attrib['id']\n",
    "                        nodes1[\"value\"] = make_areacode_out_of_postcode(child.attrib['v']).strip()\n",
    "                        tags.append(nodes1)\n",
    "                else:\n",
    "                    nodes['value'] = child.attrib['v']\n",
    "                    tags.append(nodes)\n",
    "\n",
    "            elif \"postal_code\" in child.attrib['k'].lower():\n",
    "                nodes['type'] = 'regular'\n",
    "                nodes['key'] = child.attrib['k']\n",
    "                nodes['id'] = element.attrib['id']\n",
    "                nodes[\"value\"] = update_postcode(child.attrib['v'])\n",
    "                tags.append(nodes)\n",
    "                if make_areacode_out_of_postcode(child.attrib['v']):\n",
    "                    nodes1['type'] = 'addr'\n",
    "                    nodes1['key'] = 'Areacode'\n",
    "                    nodes1['id'] = element.attrib['id']\n",
    "                    nodes1[\"value\"] = make_areacode_out_of_postcode(child.attrib['v']).strip()\n",
    "                    tags.append(nodes1)\n",
    "\n",
    "            else:\n",
    "                nodes['type'] = 'regular'\n",
    "                nodes['key'] = child.attrib['k']\n",
    "                nodes['id'] = element.attrib['id']\n",
    "                nodes['value'] = child.attrib['v']\n",
    "                tags.append(nodes)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for attribute in WAY_FIELDS:\n",
    "            #if key exists\n",
    "            if element.attrib.get(attribute):\n",
    "                way_attribs[attribute] = element.attrib[attribute]\n",
    "            else:\n",
    "                #insert placeholder value for missing data\n",
    "                node_attribs[attribute] = \"999999999999\"\n",
    "        #initialise counter to prevent bug occuring if position is reset to zero within the loop\n",
    "        position = 0\n",
    "        for child in element:\n",
    "            wayn = {}\n",
    "            wayt = {}\n",
    "            if child.tag == 'nd':\n",
    "                #create the entry in the dictionary wayn\n",
    "                if element.attrib['id'] not in way_nodes:\n",
    "                    wayn['position'] = position\n",
    "                    wayn['id'] = element.attrib['id']\n",
    "                    wayn['node_id'] = child.attrib['ref']\n",
    "                    #we are adding the whole dictionary just created into the way_nodes list using append\n",
    "                    way_nodes.append(wayn)\n",
    "                    position += 1\n",
    "                #however if there is already an associated node for this way, increment position and add as normal\n",
    "                else:\n",
    "                    wayn['position'] = position\n",
    "                    wayn['id'] = element.attrib['id']\n",
    "                    wayn['node_id'] = child.attrib['ref']\n",
    "                    way_nodes.append(wayn)\n",
    "                    position += 1\n",
    "            elif child.tag == 'tag':\n",
    "                problem = PROBLEMCHARS.match(child.attrib['k'])\n",
    "                lowco = LOWER_COLON.match(child.attrib['k'])\n",
    "                if problem:\n",
    "                    continue\n",
    "                elif lowco:\n",
    "                    wayt['type'] = child.attrib['k'].split(':', 1) [0]\n",
    "                    wayt['key'] = child.attrib['k'].split(':', 1) [1]\n",
    "                    wayt['id'] = element.attrib['id']\n",
    "                    wayt['value'] = child.attrib['v']\n",
    "                    tags.append(wayt)\n",
    "                else:\n",
    "                    wayt['type'] = 'regular'\n",
    "                    wayt['key'] = child.attrib['k']\n",
    "                    wayt['id'] = element.attrib['id']\n",
    "                    wayt['value'] = child.attrib['v']\n",
    "                    tags.append(wayt)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "\n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "def update_postcode(postcode):\n",
    "    #strips out all non alphanumeric\n",
    "    x=re.sub(r'\\W+',\"\", postcode)\n",
    "    if len(x) == 3:\n",
    "        postcode = x + \"---\"\n",
    "#adds dashes to show that the full postcode is incomplete\n",
    "    return postcode\n",
    "\n",
    "def make_areacode_out_of_postcode(postcode):\n",
    "    #create matchobject giving first 2 or 3 characters\n",
    "    m = regex3.match(postcode)\n",
    "    #use group method to convert matchobject into a string\n",
    "    areacode = m.group(0)\n",
    "    return areacode\n",
    "\n",
    "\n",
    "def update_phone(number):\n",
    "    goodmatch = regex1.match(number)\n",
    "    if not goodmatch:\n",
    "        #strips out all non digit values\n",
    "        x=re.sub(r'\\D+',\"\", number)\n",
    "        #handle numbers with 44 at the start\n",
    "        if len(x) == 12 and x.startswith(\"44\"):\n",
    "            #insert punctuation\n",
    "            number = \"+\" + x[0:2] + \"-\" + x[2:4] + \"-\" + x[5:7] + \"-\" + x[8:11]\n",
    "        #numbers in the 0141 format\n",
    "        elif len(x) == 11 and x.startswith(\"0\"):\n",
    "        #drop the 0\n",
    "            number = \"+44\" + \"-\" + x[1:3] + \"-\" + x[4:6] + \"-\" + x[7:10]            \n",
    "        #numbers including both 00 and 44\n",
    "        elif len(x) == 14 and x.startswith(\"0044\"):\n",
    "            y = str(list(x)[2:])\n",
    "            number = \"+\" + y[0:2] + \"-\" + y[2:4] + \"-\" + y[5:7] + \"-\" + y[8:11]\n",
    "\n",
    "        #numbers including 0 and 44\n",
    "        elif len(x) == 13 and x.startswith(\"044\"):\n",
    "            y = str(list(x)[1:])\n",
    "            number = \"+\" + y[0:2] + \"-\" + y[2:4] + \"-\" + y[5:7] + \"-\" + y[8:11]\n",
    "\n",
    "        #as above but in reverse order\n",
    "        elif len(x) == 13 and x.startswith(\"440\"):\n",
    "            del list(x)[2]\n",
    "            number = \"+\" + x[0:2] + \"-\" + x[2:4] + \"-\" + x[5:7] + \"-\" + x[8:11]\n",
    "            \n",
    "        elif len(x) > 21:\n",
    "            #note this as potential future improvement - write function to split out the second number and add it as a\n",
    "            #secondary phone tag\n",
    "            return\n",
    "\n",
    "    return number\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping.keys():\n",
    "            #print 'Before: ' , name\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "            #print 'After: ', name\n",
    "    return name\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Crescent\", \"Gate\", \"Gardens\", \"Path\", \"Terrace\", \"Way\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Road,\": \"Road\",\n",
    "            \"Sreet\": \"Street\",\n",
    "            \"Strret\": \"Street\",\n",
    "            \"street\" : \"Street\",\n",
    "            \"road\": \"Road\"\n",
    "            }\n",
    "\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "regex1 = re.compile(r'\\+44\\-\\d{3}\\-\\d{3}\\-\\d{4}')\n",
    "\n",
    "regex2 = re.compile(r'^[gG]\\d\\d?[ _-]?\\d\\w\\w')\n",
    "\n",
    "regex3 = re.compile(r'^[gG]\\d\\d?')\n",
    "\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Problems encountered:\n",
    "\n",
    "1. There were issues with missing data causing the reader to crash - this necessitated the use of:\n",
    "\n",
    "```\n",
    "else:\n",
    "                #insert placeholder value for missing data\n",
    "                node_attribs[attribute] = \"999999999999\"\n",
    "```\n",
    "\n",
    "to ensure that the reader did not crash when encountering missing info, and instead returned a placeholder. This is designed to be easily identifiable.\n",
    "\n",
    "2. My attempts to create a new element called Areacode based on the first three letters of any postcode were stymied because the values for postcode were being overwritten. This was solved with assistance from the forums and the use of a new dictionary , nodes1, that allowed the new element to be created in the csv for each node with a postcode.\n",
    "\n",
    "```\n",
    "elif child.attrib['k'] == \"addr:postcode\":\n",
    "                    nodes[\"value\"] = update_postcode(child.attrib['v'])\n",
    "                    tags.append(nodes)\n",
    "                    if make_areacode_out_of_postcode(child.attrib['v']):\n",
    "                        nodes1['type'] = 'addr'\n",
    "                        nodes1['key'] = 'Areacode'\n",
    "                        nodes1['id'] = element.attrib['id']\n",
    "                        nodes1[\"value\"] = make_areacode_out_of_postcode(child.attrib['v']).strip()\n",
    "                        tags.append(nodes1)                    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing CSV to database with required schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = \"C:\\Users\\James2SxyBoogaloo\\Google Drive\\Data\\P3 -  Data Wrangling\\database.db\"\n",
    "#connect to database\n",
    "conn =sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur= conn.cursor()\n",
    "\n",
    "# NODES_TAGS****************************************************************************\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "conn.commit()\n",
    "\n",
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key STRING, value STRING,type STRING)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Read in the csv file as a dictionary, format the\n",
    "# data as a list of tuples:\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close\n",
    "\n",
    "#Nodes**************************************************************************************************************\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes(id INTEGER, lat FLOAT, lon FLOAT, user STRING, uid INTEGER, version STRING, changeset INTEGER, timestamp STRING)\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['lat'].decode(\"utf-8\"), i['lon'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#WAYS**************************************************************************************************************\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways(id INTEGER, user STRING, uid INTEGER, version STRING, changeset INTEGER, timestamp STRING)\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "conn.close\n",
    "\n",
    "#WAYS_NODES**************************************************************************************************************\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_nodes(id INTEGER, node_id INTEGER, position INTEGER)\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['node_id'].decode(\"utf-8\"), i['position'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "conn.close\n",
    "\n",
    "\n",
    "#WAYS_TAGS**************************************************************************************************************\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_tags(id INTEGER, key STRING, value STRING, type STRING)\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - SQL analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_file = \"C:\\Users\\James2SxyBoogaloo\\Google Drive\\Data\\P3 -  Data Wrangling\\database.db\"\n",
    "#connect to database\n",
    "conn =sqlite3.connect(sqlite_file)\n",
    "\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top ten cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'chinese', 70), (u'indian', 66), (u'italian', 49), (u'sandwich', 40), (u'fish_and_chips', 31), (u'pizza', 21), (u'burger', 18), (u'coffee_shop', 18), (u'regional', 16), (u'asian', 10)]\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT value, count(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "WHERE key ='cuisine' \\\n",
    "GROUP BY value \\\n",
    "ORDER BY num desc \\\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "def cuisine():\n",
    "    c.execute(query)\n",
    "    results = c.fetchall()\n",
    "    print results\n",
    "\n",
    "cuisine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese narrowly edges out Indian as glasgow's preferred cuisine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top ten amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'bicycle_parking', 556), (u'post_box', 458), (u'fast_food', 307), (u'restaurant', 268), (u'pub', 255), (u'cafe', 229), (u'telephone', 166), (u'atm', 120), (u'bench', 115), (u'recycling', 98)]\n"
     ]
    }
   ],
   "source": [
    "query2 = '''\n",
    "SELECT value, count(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "WHERE key ='amenity' \\\n",
    "GROUP BY value \\\n",
    "ORDER BY num desc \\\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "def amenity():\n",
    "    c.execute(query2)\n",
    "    results = c.fetchall()\n",
    "    print results\n",
    "\n",
    "amenity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glasgow benefits from a surfeit of bicycle parking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top ten shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'convenience', 231), (u'hairdresser', 119), (u'clothes', 65), (u'supermarket', 65), (u'bookmaker', 54), (u'newsagent', 50), (u'beauty', 40), (u'bakery', 32), (u'yes', 27), (u'butcher', 24)]\n"
     ]
    }
   ],
   "source": [
    "query3 = '''\n",
    "SELECT value, count(*) as num \\\n",
    "FROM nodes_tags \\\n",
    "WHERE key ='shop' \\\n",
    "GROUP BY value \\\n",
    "ORDER BY num desc \\\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "def shop():\n",
    "    c.execute(query3)\n",
    "    results = c.fetchall()\n",
    "    print results\n",
    "\n",
    "shop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the overwhelming majority of shops in glasgow are convenience stores. There is a surprisingly low number of bookmakers - as this is a proxy for deprivation it would be interesting to explore this further, perhaps by areacode. Outside my flat there are 3 bookkeepers within a 60 second walk.\n",
    "\n",
    "Note that 'Yes' is getting returned as a type of shop - given that the existence of the shop key implies that a shop exists, this is a potential further opportunity for data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areacodes with the most Bookmakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'G1', 3), (u'G2', 1), (u'G4', 1)]\n"
     ]
    }
   ],
   "source": [
    "query4 = '''\n",
    "SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value= 'bookmaker') i \n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key = 'Areacode'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "def ACB():\n",
    "    c.execute(query4)\n",
    "    results = c.fetchall()\n",
    "    print results\n",
    "\n",
    "ACB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our query returns only 5 results. This is disappointing given the trouble it took to create the Areacode key.\n",
    "\n",
    "An examination of the CSV indicates that the vast majority of the bookmakers listed in the data do not have an associated postcode: as a result neither do they have an Areacode. - This isn't a cleaning issue however.\n",
    "\n",
    "In the interest of getting a meaningful output lets investigate restaurants - perhaps these will have fully completed postcodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areacodes with the most restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'G1', 13), (u'G2', 7), (u'G3', 6), (u'G12', 3), (u'G31', 3), (u'G4', 2), (u'G41', 2), (u'G53', 2), (u'G20', 1), (u'G33', 1)]\n"
     ]
    }
   ],
   "source": [
    "query5 = '''\n",
    "SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value= 'restaurant') i \n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key = 'Areacode'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "def AMR():\n",
    "    c.execute(query5)\n",
    "    results = c.fetchall()\n",
    "    print results\n",
    "\n",
    "AMR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we are challenged by the incompleteness of the data. Despite the previous count showing a total of no less than 268 restaurants, our top ten Areacodes only account for a small proportion of this total. However assuming that the likelihood of the restuarant not having an associated postcode (and thus Areacode) is unaffected by location, we can still conclude that G1 must have a significant proportion of the total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, there are a few issues about completeness of the data as it relates to the postcodes being completed for each node. This meant that our ability to fully utilise the newly created Areacode key to assist our SQL exploration was impaired.\n",
    "\n",
    "I noticed during my exploration that the bus stops in the data get their information directly from the NAPTAN (government public transport) database, and potentially using a similar tool -such as the post offices publically available database - could be used to add in these postcode values. There wouldn't be any danger of overriding the wrong values as the post office have the most accurate postcodes and these do not change even if a restaurant changes ownership for example. By automating this, some of the pitfalls of relying on user entry data can be avoided.\n",
    "\n",
    "Additional improvements could be made within the existing dataset.\n",
    "\n",
    "Key - Shop value - \"yes\" - this could be addressed by writing code that detects whether there is a secondary key confirming what type of shop it is. This key could then be removed, and used as the value for the Shop key, ovewrwriting 'yes'.\n",
    "\n",
    "Phone numbers - certain nodes have multiple phone numbers - This could be addressed by splitting out the second number and reassigning it to another key. I did not code a solution to this as this was not a common issue in the data and the complexity of code required to make this compatible with the treatment of every other phone number made it unrealistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
